{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Library Imports \nimport pandas as pd\nimport tensorflow as tf\nimport pickle\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport keras\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss, accuracy_score, precision_score, recall_score, f1_score\nimport scikitplot\nfrom keras.models import Model, Sequential\nfrom keras.layers import *\nfrom keras.optimizers import Adam, SGD\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.utils import plot_model\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau,EarlyStopping\n\nprint(keras.__version__)","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"2.3.1\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/til2020/TIL_NLP_train_dataset.csv')\npredict_df = pd.read_csv('/kaggle/input/til2020/TIL_NLP_test_dataset.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for null\nprint(\"There are \", train_df.isnull().sum().sum(), \"null values in the dataset\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = ['outwear','top','trousers','women dresses','women skirts']\n\nfor label in labels:\n    print('Count:' + str(train_df[label].value_counts()[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reps = [5 if val==1 else 0 for val in train_df['women dresses']]\ndresses_df = train_df.loc[np.repeat(train_df.index.values, reps)]\ntrain_df = pd.concat([train_df,dresses_df.sample(n=2000)])\n\nreps = [5 if val==1 else 0 for val in train_df['women skirts']]\nskirts_df = train_df.loc[np.repeat(train_df.index.values, reps)]\ntrain_df = pd.concat([train_df,skirts_df.sample(n=2000)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = ['outwear','top','trousers','women dresses','women skirts']\n\nfor label in labels:\n    print('Count:' + str(train_df[label].value_counts()[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split Data\nX_train, X_test, y_train, y_test = train_test_split(train_df.word_representation.values,\n                                                    train_df[['outwear','top','trousers','women dresses','women skirts']].values,\n                                                    test_size=0.2, \n                                                    stratify=train_df[['outwear','top','trousers','women dresses','women skirts']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Instantiate tokenizer\ntokenizer = text.Tokenizer(num_words=None)\nmax_len = 30\ntokenizer.fit_on_texts(list(xtrain) + list(xval))\n\n#Change train tests to sequences\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\n\n#Change train sequences to pad\nX_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n\nword_index = tokenizer.word_index\n\nX_predict = list(predict_df[\"word_representation\"])\nX_predict = tokenizer.texts_to_sequences(X_predict)\nX_predict = sequence.pad_sequences(X_predict, maxlen=max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\nwith open(\"/kaggle/input/til2020/word_embeddings.pkl\", \"rb\" ) as p:\n    word_embeddings = pickle.load(p)\n    \nembedding_matrix = np.zeros((len(word_index) + 1, 100)) # As each array for each word has 100 values\nfor word, i in word_index.items():         #presumably words we actually use in our dataset\n    embedding_vector = word_embeddings.get(word) #get vectors for words we are using from Glove dataset\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token_inputs = Input(shape=(maxlen,))\n\nembedding_layer = Embedding(len(tokenizer.word_index) + 1, \n                            100, \n                            weights=[embedding_matrix], \n                            trainable=True)(token_inputs)\n\n#Add Spatial Dropout 1D Layer\nx = SpatialDropout1D(0.3)(embedding_layer)\n\n#Original\n# x = LSTM(64)(x)\n# x = Dropout(0.7)(x)\n# x = Dense(32, activation='relu')(x)        #Best so far: LSTM 64, Dropout 0.5, Dense 64\n\n#Bidirectional LSTM\n# x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n# x = GlobalMaxPool1D()(x)\n# x = Dense(50, activation=\"relu\")(x)\n# x = Dropout(0.1)(x)\n\n#Bidirectional LSTM with two dense layers\n# x = Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3))(x)\n# x = Dense(1024, activation='relu')(x)\n# x = Dropout(0.8)(x)\n# x = Dense(1024, activation='relu')(x)\n# x = Dropout(0.8)(x)\n\n\n#Additional LSTM + Dropout\n# x = LSTM(128, return_sequences=True)(x)\n# x = Dropout(0.5)(x)\n# x = LSTM(64)(x)\n# x = Dropout(0.5)(x)\n# x = Dense(32, activation='relu')(x)\n\n#GRU\n# x = GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)(x)\n# x = GRU(300, dropout=0.3, recurrent_dropout=0.3)(x)\n# x = Dense(1024, activation='relu')(x)\n# x = Dropout(0.8)(x)\n# x = Dense(1024, activation='relu')(x)\n# x = Dropout(0.8)(x)\n\noutputs = Dense(5, activation='sigmoid')(x)\nmodel = Model(inputs=token_inputs, outputs=outputs)\n\nmodel.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy', 'binary_crossentropy'])\n#model.load_weights('/content/drive/My Drive/Colab Notebooks/TIL2020 Actual/jackpot_checkpoint')\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualise model architecture\nplot_model(model, to_file='model_plot4a.png', show_shapes=True, show_layer_names=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train model for N epochs. Vary as you desire\nhistory = model.fit(X_train, \n                    y_train, \n                    batch_size=64, \n                    epochs=100, \n                    verbose=1, \n                    validation_data=(X_test, y_test),\n                    callbacks = [\n                        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, min_lr = 0.00001),\n                        EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto'),\n                        ModelCheckpoint(filepath='model-simple.h5',verbose = 1, save_best_only=True, mode = 'auto')\n                   ])\n\n#rename the filepath accordingly.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print accuracy \n\nscore = model.evaluate(X_test, y_test, verbose=1)\n\nprint(\"Test Score:\", score[0])\nprint(\"Test Accuracy:\", score[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probas = model.predict(X_test)\n\npred_indices = np.argmax(probas, axis=1)\nclasses = np.array(range(1, 6))\npreds = classes[pred_indices]\nprint('Log loss: {}'.format(log_loss(classes[np.argmax(y_test, axis=1)], probas)))\nprint('Accuracy: {}'.format(accuracy_score(classes[np.argmax(y_test, axis=1)], preds)))\nscikitplot.metrics.plot_confusion_matrix(classes[np.argmax(y_test, axis=1)], preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nplt.style.use(\"ggplot\")\n\n# list all data in history\nprint(history.history.keys())\n\n# summarize history for AUC\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import GridSearchCV\n\nclf = OneVsRestClassifier(XGBClassifier(n_jobs=-1, max_depth=4))\n\n# You may need to use MultiLabelBinarizer to encode your variables from arrays [[x, y, z]] to a multilabel \n# format before training.\nmlb = MultiLabelBinarizer()\ny_train = mlb.fit_transform(y_train)\n\nclf.fit(X_train, y_train)\n\npredictions = clf.predict_proba(X_test)\n\nprint('Log loss: {}'.format(log_loss(y_test, predictions)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(X_predict)\nprediction_df = pd.DataFrame(predictions)\nprediction_df.columns = [\"outwear\", \"top\", \"trousers\", \"women dresses\", \"women skirts\"]\nprediction_df.index.name = 'id'\nprediction_df = prediction_df.round().astype(int)\nprediction_df.to_csv('predictions.csv')\nprediction_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the weights\n#model.save_weights('/content/drive/My Drive/Colab Notebooks/TIL2020 Actual/jackpot_checkpoint')\n\n# Restore the weights\n# model.load_weights('./checkpoints/my_checkpoint')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kaggle competitions submit -c til2020 -f submission.csv -m \"Message\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}